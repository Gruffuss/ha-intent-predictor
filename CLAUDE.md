# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## ‚ö†Ô∏è CRITICAL DEVELOPMENT RULES

**1. NEVER MAKE CODE CHANGES WITHOUT EXPLICIT PERMISSION**
- Always ask before modifying existing functions, classes, or parameters
- Never simplify, alter, or "improve" code without user approval  
- Only fix import errors and references when explicitly requested
- Preserve all existing functionality and architecture

**2. ALWAYS CHECK IF FILES/FEATURES ALREADY EXIST BEFORE CREATING ANYTHING NEW**
- Use `find`, `ls`, `grep`, or `Read` tools to check existing files
- Check repository structure before implementing
- Avoid duplicating existing functionality
- Build upon existing codebase, don't recreate

## Project Overview

This is an adaptive ML-based occupancy prediction system for Home Assistant that learns patterns without hardcoded assumptions. It provides 2-hour preheating and 15-minute precooling predictions using ensemble learning and real-time adaptation.

**Core Philosophy**: Pure learning with NO assumptions about occupancy patterns or schedules.

## Technology Stack

- **Python 3.9+** with asyncio-based architecture
- **River** for online machine learning (primary ML framework)
- **TimescaleDB** for time-series sensor data storage
- **Redis** for feature caching and real-time data
- **Apache Kafka** for event streaming
- **FastAPI** for REST API and Home Assistant integration
- **Docker** for infrastructure services
- **Prometheus + Grafana** for monitoring

## Development Commands

### Environment Setup
```bash
# Clone and setup virtual environment
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

pip install -r requirements.txt
```

### Database Setup
```bash
# Start infrastructure services
docker-compose up -d timescaledb redis kafka

# Initialize database schema
psql -h localhost -U postgres -f database/schema.sql
```

### Running the Application
```bash
# Development mode (main entry point)
python main.py

# Production deployment
sudo python scripts/deploy.py install
sudo systemctl start ha-intent-predictor
```

### Bootstrap Commands (MUST use venv in container)
```bash
# ‚úÖ COMPLETED - Complete system bootstrap with historical data import 
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && python scripts/bootstrap_complete.py'

# ‚úÖ COMPLETED - Initial historical data import (180 days) - 1,142,674 events imported
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && python scripts/historical_import.py'

# Simple fallback import if main historical import fails
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && python scripts/simple_historical_import.py'
```

### Deployed System Access (Container ID 200 on Proxmox)

**Running Location**: Proxmox LXC Container
- **Proxmox Host**: Intel N200, 4 cores, 8GB RAM
- **Container ID**: 200
- **IP Address**: `192.168.51.10`
- **Hostname**: `ha-predictor`

**SSH Connection Setup**:
- **SSH Config**: `~/.ssh/config` contains `ha-predictor` host entry
- **Private Key**: `~/.ssh/ha-predictor` 
- **Authentication**: Key-based (no password required)
- **Container User**: `root`
- **Backup Password**: `hapredictor123` (use key auth preferred)

**Direct Connection Commands**:
```bash
# SSH into container directly (primary method)
ssh ha-predictor

# Alternative: SSH with explicit key
ssh -i ~/.ssh/ha-predictor root@192.168.51.10

# From Proxmox host: Enter container directly
pct enter 200

# Monitor system status (password-free)
ssh ha-predictor '/opt/ha-intent-predictor/scripts/remote-monitor.sh show'

# Get JSON status for programmatic access
ssh ha-predictor '/opt/ha-intent-predictor/scripts/remote-monitor.sh json'

# Check Docker services
ssh ha-predictor 'cd /opt/ha-intent-predictor && docker compose ps'

# View service logs
ssh ha-predictor 'cd /opt/ha-intent-predictor && docker compose logs --tail 20'

# Restart services if needed
ssh ha-predictor 'cd /opt/ha-intent-predictor && docker compose restart'
```

**Service Endpoints**:
- **PostgreSQL**: `192.168.51.10:5432` (user: `ha_predictor`, db: `ha_predictor`, password: `hapredictor_db_pass`)
- **Redis**: `192.168.51.10:6379`
- **Kafka**: `192.168.51.10:9092`
- **Zookeeper**: `192.168.51.10:2181`
- **Web Health Check**: `http://192.168.51.10/health`

### Testing (MUST use venv in container)
```bash
# Run installation tests
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && python deploy/testing/test_installation.py'

# Test database connection
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && python -c "from config.config_loader import ConfigLoader; print(ConfigLoader().get(\"home_assistant\"))"'

# Check current system status
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && curl http://192.168.51.10/health'

# Verify historical data
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && python -c "
from src.storage.timeseries_db import TimescaleDBManager
from config.config_loader import ConfigLoader
import asyncio

async def check():
    config = ConfigLoader()
    db_config = config.get(\"database.timescale\")
    db = TimescaleDBManager(f\"postgresql+asyncpg://{db_config[\"user\"]}:{db_config[\"password\"]}@{db_config[\"host\"]}:{db_config[\"port\"]}/{db_config[\"database\"]}\")
    await db.initialize()
    async with db.engine.begin() as conn:
        from sqlalchemy import text
        result = await conn.execute(text(\"SELECT COUNT(*) FROM sensor_events\"))
        print(f\"Events in database: {result.fetchone()[0]:,}\")
    await db.close()

asyncio.run(check())
"'
```

### Monitoring
- **API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health  
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (admin/admin)
- **Kafka UI**: http://localhost:8080

## Architecture Overview

### Core Components (src/)

**Storage Layer** (`src/storage/`)
- `timeseries_db.py`: TimescaleDB manager for sensor data
- `feature_store.py`: Redis-based feature caching
- `model_store.py`: Model versioning and persistence

**Learning Layer** (`src/learning/`)
- `adaptive_predictor.py`: Main ML engine with ensemble learning
- `online_models.py`: River-based online learning models
- `pattern_discovery.py`: Temporal pattern mining
- `anomaly_detection.py`: Cat activity and sensor validation

**Ingestion Layer** (`src/ingestion/`)
- `ha_stream.py`: Home Assistant data streaming
- `event_processor.py`: Real-time event processing
- `kafka_consumer.py`: Kafka event consumption

**Integration Layer** (`src/integration/`)
- `api.py`: FastAPI REST endpoints
- `ha_publisher.py`: Dynamic Home Assistant entity creation
- `monitoring.py`: System metrics and health

**Prediction Layer** (`src/prediction/`)
- `resilient_predictor.py`: Production-ready prediction service
- `bathroom_predictor.py`: Inference-based bathroom occupancy
- `horizon_optimizer.py`: Multi-horizon prediction optimization

**Adaptation Layer** (`src/adaptation/`)
- `drift_detection.py`: Model drift monitoring
- `performance_monitor.py`: Performance tracking
- `hyperopt.py`: Hyperparameter optimization

### Configuration Structure

**Main Config** (`config/`)
- `config_loader.py`: Configuration management with dot notation access
- `sensors.yaml`: All 98 sensors mapped to rooms/zones
- `rooms.yaml`: Room structure with full/subzone relationships

**Environment Config** (`.env` from `systemd/environment.template`)
```bash
HA_URL=http://homeassistant.local:8123
HA_TOKEN=your_long_lived_access_token
DB_PASSWORD=hapredictor_db_pass
```

### Room Architecture

**Multi-Zone Rooms**:
- Living/Kitchen: Dual full zones + subzones (couch, stove, sink, dining) - Combined as `living_kitchen`
- Bedroom: Full zone + person-specific bed sides (Anca/Vladimir)  
- Office: Full zone + dual desks (Anca/Vladimir)

**Bathroom Inference**: Entry/exit sensors with door states (no interior sensors)

**Person-Specific Detection**: Individual patterns for Anca and Vladimir

**Home Assistant Integration**:
- HA URL: `http://192.168.51.247:8123`
- Timezone: Europe/Bucharest (GMT+3)
- HA sends timestamps in UTC (correct format)
- ~98 sensors total across presence, doors, climate, light

## Data Flow

```
Home Assistant ‚Üí Kafka Queue ‚Üí Event Processor ‚Üí TimescaleDB
                                       ‚Üì
Redis Features ‚Üê Adaptive ML Engine ‚Üê Feature Engineering
     ‚Üì
FastAPI ‚Üê Prediction Service ‚Üí Home Assistant Entities
```

## Key Files and Entry Points

- `main.py`: Main application orchestrator and entry point
- `scripts/bootstrap_complete.py`: Complete system initialization
- `config/config_loader.py`: Configuration management (supports `config.get('path.to.value')`)
- `src/learning/adaptive_predictor.py`: Core ML prediction engine
- `src/integration/ha_publisher.py`: Home Assistant integration
- `docker-compose.yml`: Infrastructure services definition

## Learning Process

1. **Historical Import**: 180 days of sensor data via `scripts/historical_import.py`
2. **Pattern Discovery**: Automatic temporal pattern mining
3. **Feature Engineering**: Dynamic feature generation without assumptions
4. **Model Selection**: Ensemble of online learning algorithms
5. **Continuous Adaptation**: Real-time learning from new observations
6. **Anomaly Detection**: Physics-based cat activity detection

## Troubleshooting

### Common Issues and Solutions

**Import Errors** (Most Common):
- Problem: `cannot import name 'TimeSeriesDB'` 
- Solution: Use correct class names: `TimescaleDBManager`, `RedisFeatureStore`
- Always check actual class names with `Grep` before assuming

**Timezone Errors** (CRITICAL):
- Problem: "can't compare offset-naive and offset-aware datetimes"
- Solution: Always use `datetime.now(timezone.utc)` not `datetime.now()`
- This is critical, not minor - will massively skew predictions

**Missing Historical Data** (RESOLVED):
- Problem: Models show 0.0 accuracy, immediate drift detection
- Root Cause: No 180 days of historical data for bootstrapping
- ‚úÖ **CURRENT STATUS**: 1,142,674 events imported over 180 days (COMPLETE)

**Database Issues**: 
- Check `docker-compose logs timescaledb` and ensure schema is loaded
- Verify PostgreSQL service is running: `systemctl status postgresql`

**HA Connection**: 
- Verify HA_URL and HA_TOKEN in environment
- Test: `curl -H "Authorization: Bearer YOUR_TOKEN" http://YOUR_HA_URL/api/states`

**Performance**: Monitor via Prometheus metrics at localhost:9090

**Logs**: 
- Main application: `logs/ha_predictor.log` 
- Container logs: `ssh ha-predictor 'tail -f /opt/ha-intent-predictor/main_app_fixed.log'`
- System logs: `journalctl -u ha-intent-predictor -f`

### Configuration Files Location
- **Container Config**: `/opt/ha-intent-predictor/config/system.yaml` (exists in container, gitignored locally)
- **Local Config**: Use `config/config_loader.py` for configuration management
- **Environment**: Set via `systemd/environment.template` ‚Üí `.env`

## Development Notes

### Code Architecture
- Uses async/await throughout for high-performance concurrent processing
- All ML models use online learning (no batch retraining)
- Configuration uses dot notation: `config.get('database.timescale.host')`
- Error handling follows graceful degradation patterns
- Docker services have health checks and automatic restart policies

### Critical Class Name Mappings (Common Import Issues)
**WRONG NAMES** ‚Üí **CORRECT NAMES**:
- `TimeSeriesDB` ‚Üí `TimescaleDBManager`
- `FeatureStore` ‚Üí `RedisFeatureStore`  
- `DataEnricher` ‚Üí `DynamicFeatureDiscovery`

### Deployment Workflow
1. **Make changes locally**
2. **Commit to git**: Use proper commit format with Claude Code attribution
3. **Push to GitHub**: `git push origin working-fixes`
4. **Pull to container**: `ssh ha-predictor 'cd /opt/ha-intent-predictor && git pull origin working-fixes'`
5. **Test in container**: Always use container's virtual environment

### Working in Container (CRITICAL - Always Use Virtual Environment)
**The container is running in venv** - ALL Python commands must activate the virtual environment first.

```bash
# ‚ö†Ô∏è CRITICAL: Always work in container's virtual environment
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && [command]'

# Check if already running (also use venv for Python scripts)
ssh ha-predictor 'ps aux | grep main.py'

# Start system (MUST use venv)
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && python main.py'

# Monitor logs
ssh ha-predictor 'cd /opt/ha-intent-predictor && tail -f main_app_fixed.log'

# Example: Check database connection (requires venv)
ssh ha-predictor 'cd /opt/ha-intent-predictor && source venv/bin/activate && python -c "from src.storage.timeseries_db import TimescaleDBManager; print(\"‚úì DB accessible\")"'
```

**Why Virtual Environment is Critical**:
- All Python dependencies are installed in `/opt/ha-intent-predictor/venv/`
- Without `source venv/bin/activate`, import errors will occur
- System Python lacks required ML libraries (River, pandas, psycopg2, etc.)
- **EVERY Python command must include the venv activation**

### Current System Status (July 22, 2025 - 16:08)

**üéâ BOOTSTRAP COMPLETED SUCCESSFULLY - ALL ERRORS FIXED**:
- **Database**: ‚úÖ PERFECT - Complete schema.sql implementation with zero errors
- **Schema Compliance**: ‚úÖ EXACT match to setup_instructions.md database/schema.sql
- **Bootstrap Process**: ‚úÖ COMPLETED SUCCESSFULLY (16:08) - All components operational
- **Critical Lesson**: ‚úÖ APPLIED - Never downplay errors, they prevent system functionality

**üîß COMPLETE SCHEMA FIXES APPLIED (ZERO ERRORS)**:
1. **Perfect TimescaleDB Hypertables** - ALL 7 hypertables created successfully:
   - sensor_events, computed_features, occupancy_predictions, room_occupancy ‚úÖ
   - model_performance, anomaly_events, system_metrics ‚úÖ
2. **All Indices Working** - ALL 28 indices created successfully (including critical idx_model_performance_drift)
3. **Complete Table Schema** - All tables match schema.sql exactly:
   - sensor_events: Complete with person_specific, activity_type, zone_type, zone_name, anomaly fields
   - discovered_patterns: Correct table name (not pattern_discoveries) 
   - model_performance: Added missing drift_detected column + all schema.sql columns
   - anomaly_events, system_metrics, model_configurations: Complete implementation
4. **TimescaleDB Compatibility** - Fixed PRIMARY KEY conflicts with hypertable partitioning
5. **Pattern Discovery Fixed** - Statistical test error resolved (str object cannot be interpreted as integer)

**üìä FINAL BOOTSTRAP SUCCESS LOG (16:08)**:
```
‚úÖ ALL 7 TimescaleDB hypertables created successfully
‚úÖ ALL 28 indices created successfully (no failures)
‚úÖ TimescaleDB initialized with complete schema.sql structure
‚úÖ All ML models initialized for 5 rooms (living_kitchen, bedroom, office, bathroom, small_bathroom)
‚úÖ Both prediction horizons working (15min and 120min) 
‚úÖ Adaptive learning components fully operational
‚úÖ Pattern discovery system functional
‚úÖ Anomaly detection system operational  
‚úÖ Performance monitoring system active
‚úÖ All storage layers working (TimescaleDB, Redis, ModelStore)
üîÑ Historical import ready to begin
```

**‚ö†Ô∏è CRITICAL SUCCESS FACTORS**:
- **Schema Conformance**: 100% compliance with setup_instructions.md - NO shortcuts or simplifications
- **Error Elimination**: Every error fixed completely - errors prevent system functionality
- **Database Integrity**: Perfect TimescaleDB implementation with all hypertables and indices

**üìã NEXT SESSION ACTIONS**:
1. **Start Historical Import** - 180-day data import with corrected room assignment
2. **Verify Pattern Discovery** - Test with real data using fixed algorithms
3. **Test ML Pipeline** - End-to-end prediction validation
4. **HA Integration** - Verify Home Assistant entity creation and updates

**üèóÔ∏è CONTAINER STATUS**: Proxmox LXC 200 (192.168.51.10) - FULLY OPERATIONAL with perfect database schema