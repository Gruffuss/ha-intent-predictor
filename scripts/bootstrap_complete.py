#!/usr/bin/env python3
"""
Complete Bootstrap System - Merged from bootstrap.py and bootstrap_new.py
Implements the exact bootstrap process from CLAUDE.md with full infrastructure setup
"""

import asyncio
import logging
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List

# Add src and config to path
sys.path.append(str(Path(__file__).parent.parent / 'src'))
sys.path.append(str(Path(__file__).parent.parent))

from storage.timeseries_db import TimescaleDBManager
from storage.feature_store import RedisFeatureStore
from storage.model_store import ModelStore
from learning.pattern_discovery import PatternDiscovery
from learning.adaptive_predictor import AdaptiveOccupancyPredictor
from integration.ha_publisher import DynamicHAIntegration
from config.config_loader import ConfigLoader
from learning.anomaly_detection import AdaptiveCatDetector

logger = logging.getLogger(__name__)


class CompleteSystemBootstrap:
    """
    Complete system bootstrap merging infrastructure setup and CLAUDE.md process.
    
    Follows CLAUDE.md specifications exactly:
    - Import 180 days of historical data
    - Discover patterns for each room type
    - Initialize person-specific learning
    - Set up Home Assistant integration
    - No assumptions about patterns or schedules
    """
    
    def __init__(self, config_path: str = "config/system.yaml"):
        self.config = ConfigLoader(config_path)
        self.components = {}
        
        # Complete sensor groups from both bootstrap files (CLAUDE.md specification)
        self.sensor_groups = {
            'presence_zones': [
                'binary_sensor.presence_livingroom_full',
                'binary_sensor.presence_livingroom_couch',
                'binary_sensor.kitchen_pressence_full_kitchen',
                'binary_sensor.kitchen_pressence_stove',
                'binary_sensor.kitchen_pressence_sink',
                'binary_sensor.kitchen_pressence_dining_table',
                'binary_sensor.bedroom_presence_sensor_full_bedroom',
                'binary_sensor.bedroom_presence_sensor_anca_bed_side',
                'binary_sensor.bedroom_vladimir_bed_side',
                'binary_sensor.bedroom_floor',
                'binary_sensor.bedroom_entrance',
                'binary_sensor.office_presence_full_office',
                'binary_sensor.office_presence_anca_desk',
                'binary_sensor.office_presence_vladimir_desk',
                'binary_sensor.office_entrance',
                'binary_sensor.bathroom_entrance',
                'binary_sensor.presence_small_bathroom_entrance',
                'binary_sensor.guest_bedroom_entrance',
                'binary_sensor.presence_ground_floor_hallway',
                'binary_sensor.upper_hallway',
                'binary_sensor.upper_hallway_upstairs',
                'binary_sensor.upper_hallway_downstairs',
                'binary_sensor.presence_stairs_up_ground_floor'
            ],
            'doors': [
                'binary_sensor.bathroom_door_sensor_contact',
                'binary_sensor.bedroom_door_sensor_contact',
                'binary_sensor.office_door_sensor_contact',
                'binary_sensor.guest_bedroom_door_sensor_contact',
                'binary_sensor.small_bathroom_door_sensor_contact'
            ],
            'climate': [
                'sensor.livingroom_env_sensor_temperature',
                'sensor.livingroom_env_sensor_humidity',
                'sensor.bedroom_env_sensor_temperature',
                'sensor.bedroom_env_sensor_humidity',
                'sensor.office_env_sensor_temperature',
                'sensor.office_env_sensor_humidity',
                'sensor.bathroom_env_sensor_temperature',
                'sensor.bathroom_env_sensor_humidity',
                'sensor.guest_bedroom_env_sensor_temperature',
                'sensor.guest_bedroom_env_sensor_humidity',
                'sensor.upper_hallway_env_sensor_temperature',
                'sensor.upper_hallway_env_sensor_humidity',
                'sensor.attic_env_sensor_temperature',
                'sensor.attic_env_sensor_humidity',
                'sensor.big_bath_env_sensor_temperature',
                'sensor.big_bath_env_sensor_humidity'
            ],
            'light_levels': [
                'sensor.bedroom_presence_light_level',
                'sensor.kitchen_pressence_light_level',
                'sensor.livingroom_pressence_light_level',
                'sensor.office_presence_light_level',
                'sensor.upper_hallway_pressence_light_level'
            ]
        }
        
        logger.info("Complete system bootstrap initialized")
    
    async def bootstrap_system(self):
        """
        Complete bootstrap process implementing exact CLAUDE.md specifications
        """
        
        print("="*60)
        print("ðŸš€ HA INTENT PREDICTOR SYSTEM BOOTSTRAP")
        print("="*60)
        print("Implementing CLAUDE.md specifications:")
        print("- Import 180 days of historical data")
        print("- Discover patterns without assumptions")
        print("- Initialize person-specific learning")
        print("- Set up adaptive prediction system")
        print("="*60)
        
        try:
            # Step 1: Initialize storage infrastructure
            print("\n1. ðŸ—ï¸  Initializing storage infrastructure...")
            await self._initialize_storage()
            print("   âœ“ Storage infrastructure completed")
            
            # Step 2: Initialize learning components
            print("\n2. ðŸ§  Initializing adaptive learning components...")
            await self._initialize_learning()
            print("   âœ“ Learning components completed")
            
            # Step 3: Import historical data (CLAUDE.md core requirement)
            print("\n3. ðŸ“Š Importing 180 days of historical data...")
            await self._import_historical_data()
            
            # Step 4: Configure combined living/kitchen space (CLAUDE.md requirement)
            print("\n4. ðŸ  Configuring unified living/kitchen space...")
            await self._configure_combined_spaces()
            
            # Step 5: Discover patterns for each room type (CLAUDE.md process)
            print("\n5. ðŸ” Discovering patterns (no assumptions)...")
            await self._discover_patterns()
            
            # Step 6: Initialize person-specific learning (CLAUDE.md: Anca & Vladimir)
            print("\n6. ðŸ‘¥ Initializing person-specific learning...")
            await self._initialize_person_learning()
            
            # Step 7: Start continuous learning system (CLAUDE.md requirement)
            print("\n7. âš¡ Starting continuous learning system...")
            await self._start_continuous_learning()
            
            # Step 8: Set up Home Assistant integration (CLAUDE.md requirement)
            print("\n8. ðŸ”Œ Setting up Home Assistant integration...")
            await self._setup_ha_integration()
            
            # Step 9: Validate complete system
            print("\n9. âœ… Validating system readiness...")
            await self._validate_system()
            
            print("\n" + "="*60)
            print("ðŸŽ‰ BOOTSTRAP COMPLETED SUCCESSFULLY!")
            print("="*60)
            print("âœ… System ready - all patterns learned from observation")
            print("âœ… No schedules or patterns assumed - pure data-driven")
            print("âœ… Adaptive learning system is operational")
            print("="*60)
            
        except Exception as e:
            logger.error(f"Bootstrap failed: {e}")
            print(f"\nâŒ BOOTSTRAP FAILED: {e}")
            raise
    
    async def _initialize_storage(self):
        """Initialize complete storage infrastructure"""
        
        # Initialize TimescaleDB with proper class name
        print("  - Setting up TimescaleDB...")
        db_config = self.config.get('database.timescale')
        db_connection_string = f"postgresql+asyncpg://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}"
        self.components['timeseries_db'] = TimescaleDBManager(
            db_connection_string
        )
        await self.components['timeseries_db'].initialize()
        print("    âœ“ TimescaleDB initialized")
        
        # Create additional database schema
        print("  - Creating additional database schema...")
        await self._create_database_schema()
        print("    âœ“ Database schema created")
        
        # Initialize Redis feature store with proper class name
        print("  - Setting up Redis feature store...")
        redis_config = self.config.get('redis')
        redis_url = f"redis://{redis_config['host']}:{redis_config['port']}/{redis_config['db']}"
        self.components['feature_store'] = RedisFeatureStore(redis_url)
        await self.components['feature_store'].initialize()
        print("    âœ“ Redis feature store initialized")
        
        # Initialize model storage
        print("  - Setting up model versioning store...")
        self.components['model_store'] = ModelStore(
            self.config.get('model_storage')
        )
        await self.components['model_store'].initialize()
        print("    âœ“ Model storage initialized")
        
        print("  âœ“ Storage infrastructure initialized")
    
    async def _create_database_schema(self):
        """Create complete database schema matching schema.sql"""
        
        async with self.components['timeseries_db'].engine.begin() as conn:
            from sqlalchemy import text
            
            # Enable TimescaleDB extension
            await conn.execute(text("CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;"))
            
            # 1. Create sensor_events table - TimescaleDB compatible (no PRIMARY KEY before hypertable)
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS sensor_events (
                    id BIGSERIAL,
                    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    entity_id VARCHAR(255) NOT NULL,
                    state VARCHAR(50) NOT NULL,
                    previous_state VARCHAR(50),
                    attributes JSONB,
                    room VARCHAR(100),
                    sensor_type VARCHAR(50),
                    zone_type VARCHAR(50),
                    zone_name VARCHAR(100),
                    person_specific VARCHAR(50),
                    activity_type VARCHAR(50),
                    time_since_last_change INTERVAL,
                    state_transition VARCHAR(100),
                    concurrent_events INTEGER DEFAULT 0,
                    anomaly_score FLOAT DEFAULT 0.0,
                    is_anomaly BOOLEAN DEFAULT FALSE,
                    anomaly_type VARCHAR(50)
                );
            """))
            
            # 2. Create computed_features table - TimescaleDB compatible
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS computed_features (
                    id BIGSERIAL,
                    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    room VARCHAR(100) NOT NULL,
                    features JSONB NOT NULL,
                    feature_count INTEGER,
                    feature_names TEXT[],
                    feature_importance JSONB,
                    context_window_minutes INTEGER,
                    data_points_used INTEGER
                );
            """))
            
            # 3. Create occupancy_predictions table - TimescaleDB compatible
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS occupancy_predictions (
                    id BIGSERIAL,
                    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    room VARCHAR(100) NOT NULL,
                    horizon_minutes INTEGER NOT NULL,
                    probability FLOAT NOT NULL,
                    uncertainty FLOAT NOT NULL,
                    confidence FLOAT NOT NULL,
                    model_name VARCHAR(100),
                    model_version VARCHAR(50),
                    ensemble_weights JSONB,
                    contributing_models JSONB,
                    feature_importance JSONB,
                    top_features JSONB,
                    actual_outcome BOOLEAN,
                    prediction_error FLOAT,
                    processing_time_ms INTEGER,
                    explanation TEXT
                );
            """))
            
            # 4. Create room_occupancy table - TimescaleDB compatible
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS room_occupancy (
                    id BIGSERIAL,
                    timestamp TIMESTAMPTZ NOT NULL,
                    room VARCHAR(100) NOT NULL,
                    occupied BOOLEAN NOT NULL,
                    confidence DOUBLE PRECISION,
                    inference_method VARCHAR(100),
                    supporting_evidence JSONB,
                    person VARCHAR(50),
                    duration_minutes INTEGER,
                    processed_at TIMESTAMPTZ DEFAULT NOW()
                );
            """))
            
            # 5. Create discovered_patterns table - exact match to schema.sql
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS discovered_patterns (
                    id BIGSERIAL PRIMARY KEY,
                    discovered_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    room VARCHAR(100) NOT NULL,
                    pattern_type VARCHAR(100) NOT NULL,
                    pattern_data JSONB NOT NULL,
                    frequency INTEGER NOT NULL,
                    confidence FLOAT NOT NULL,
                    support FLOAT NOT NULL,
                    significance_score FLOAT NOT NULL,
                    time_window_minutes INTEGER,
                    valid_from TIMESTAMPTZ,
                    valid_to TIMESTAMPTZ,
                    sensor_entities TEXT[],
                    conditions JSONB,
                    prediction_improvement FLOAT,
                    usage_count INTEGER DEFAULT 0,
                    last_used TIMESTAMPTZ
                );
            """))
            
            # 6. Create anomaly_events table - TimescaleDB compatible  
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS anomaly_events (
                    id BIGSERIAL,
                    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    room VARCHAR(100),
                    anomaly_type VARCHAR(100) NOT NULL,
                    anomaly_score FLOAT NOT NULL,
                    confidence FLOAT NOT NULL,
                    sensor_sequence JSONB,
                    triggering_events JSONB,
                    explanation TEXT,
                    detector_version VARCHAR(50),
                    detection_method VARCHAR(100),
                    human_verified BOOLEAN,
                    human_feedback TEXT,
                    feedback_timestamp TIMESTAMPTZ
                );
            """))
            
            # 7. Create system_metrics table - TimescaleDB compatible
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS system_metrics (
                    id BIGSERIAL,
                    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    cpu_usage FLOAT,
                    memory_usage FLOAT,
                    disk_usage FLOAT,
                    events_processed_per_second INTEGER,
                    predictions_per_second INTEGER,
                    average_prediction_time_ms FLOAT,
                    active_models INTEGER,
                    model_updates_per_hour INTEGER,
                    feature_count INTEGER,
                    sensor_events_per_hour INTEGER,
                    data_ingestion_rate_mb_per_second FLOAT,
                    error_count INTEGER DEFAULT 0,
                    warning_count INTEGER DEFAULT 0
                );
            """))
            
            # 8. Create model_configurations table - TimescaleDB compatible
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS model_configurations (
                    id BIGSERIAL,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    room VARCHAR(100) NOT NULL,
                    model_name VARCHAR(100) NOT NULL,
                    version VARCHAR(50) NOT NULL,
                    hyperparameters JSONB NOT NULL,
                    features_used TEXT[],
                    training_config JSONB,
                    validation_accuracy FLOAT,
                    cross_validation_score FLOAT,
                    deployed_at TIMESTAMPTZ,
                    deprecated_at TIMESTAMPTZ,
                    is_active BOOLEAN DEFAULT TRUE
                );
            """))
            
            # 6. Create model_performance table - exact match to schema.sql
            await conn.execute(text("""
                CREATE TABLE IF NOT EXISTS model_performance (
                    id BIGSERIAL,
                    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    room VARCHAR(100) NOT NULL,
                    model_name VARCHAR(100) NOT NULL,
                    horizon_minutes INTEGER NOT NULL,
                    accuracy FLOAT NOT NULL,
                    precision_score FLOAT,
                    recall FLOAT,
                    f1_score FLOAT,
                    roc_auc FLOAT,
                    window_size INTEGER,
                    evaluation_period INTERVAL,
                    model_version VARCHAR(50),
                    hyperparameters JSONB,
                    training_data_points INTEGER,
                    drift_score FLOAT,
                    drift_detected BOOLEAN DEFAULT FALSE
                );
            """))
            
        # Create hypertables in separate transaction
        async with self.components['timeseries_db'].engine.begin() as conn:
            hypertables = [
                ("sensor_events", "timestamp"),
                ("computed_features", "timestamp"), 
                ("occupancy_predictions", "timestamp"),
                ("room_occupancy", "timestamp"),
                ("model_performance", "timestamp"),
                ("anomaly_events", "timestamp"),
                ("system_metrics", "timestamp")
            ]
            
            for table, time_column in hypertables:
                try:
                    await conn.execute(text(f"""
                        SELECT create_hypertable('{table}', '{time_column}',
                                               if_not_exists => TRUE,
                                               chunk_time_interval => INTERVAL '1 day');
                    """))
                    logger.info(f"{table} hypertable created successfully")
                except Exception as e:
                    logger.warning(f"{table} hypertable creation failed (may already exist): {e}")
        
        # Create all required indices from schema.sql
        async with self.components['timeseries_db'].engine.begin() as conn:
            indices = [
                # sensor_events indices
                "CREATE INDEX IF NOT EXISTS idx_sensor_events_timestamp ON sensor_events (timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_sensor_events_entity ON sensor_events (entity_id, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_sensor_events_room ON sensor_events (room, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_sensor_events_person ON sensor_events (person_specific, timestamp DESC) WHERE person_specific IS NOT NULL;",
                "CREATE INDEX IF NOT EXISTS idx_sensor_events_anomaly ON sensor_events (is_anomaly, timestamp DESC) WHERE is_anomaly = TRUE;",
                "CREATE INDEX IF NOT EXISTS idx_sensor_events_composite ON sensor_events (room, timestamp DESC, entity_id, state);",
                
                # computed_features indices
                "CREATE INDEX IF NOT EXISTS idx_computed_features_timestamp ON computed_features (timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_computed_features_room ON computed_features (room, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_computed_features_gin ON computed_features USING gin (features);",
                
                # occupancy_predictions indices
                "CREATE INDEX IF NOT EXISTS idx_occupancy_predictions_timestamp ON occupancy_predictions (timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_occupancy_predictions_room ON occupancy_predictions (room, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_occupancy_predictions_horizon ON occupancy_predictions (horizon_minutes, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_occupancy_predictions_accuracy ON occupancy_predictions (prediction_error, timestamp DESC) WHERE actual_outcome IS NOT NULL;",
                "CREATE INDEX IF NOT EXISTS idx_predictions_composite ON occupancy_predictions (room, horizon_minutes, timestamp DESC, probability);",
                
                # discovered_patterns indices  
                "CREATE INDEX IF NOT EXISTS idx_discovered_patterns_room ON discovered_patterns (room, discovered_at DESC);",
                "CREATE INDEX IF NOT EXISTS idx_discovered_patterns_type ON discovered_patterns (pattern_type, discovered_at DESC);",
                "CREATE INDEX IF NOT EXISTS idx_discovered_patterns_significance ON discovered_patterns (significance_score DESC);",
                "CREATE INDEX IF NOT EXISTS idx_discovered_patterns_gin ON discovered_patterns USING gin (pattern_data);",
                
                # model_performance indices
                "CREATE INDEX IF NOT EXISTS idx_model_performance_timestamp ON model_performance (timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_model_performance_room_model ON model_performance (room, model_name, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_model_performance_accuracy ON model_performance (accuracy DESC, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_model_performance_drift ON model_performance (drift_detected, timestamp DESC) WHERE drift_detected = TRUE;",
                "CREATE INDEX IF NOT EXISTS idx_performance_composite ON model_performance (room, model_name, horizon_minutes, timestamp DESC);",
                
                # anomaly_events indices
                "CREATE INDEX IF NOT EXISTS idx_anomaly_events_timestamp ON anomaly_events (timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_anomaly_events_type ON anomaly_events (anomaly_type, timestamp DESC);",
                "CREATE INDEX IF NOT EXISTS idx_anomaly_events_score ON anomaly_events (anomaly_score DESC);",
                "CREATE INDEX IF NOT EXISTS idx_anomaly_events_room ON anomaly_events (room, timestamp DESC) WHERE room IS NOT NULL;",
                
                # system_metrics indices
                "CREATE INDEX IF NOT EXISTS idx_system_metrics_timestamp ON system_metrics (timestamp DESC);",
                
                # model_configurations indices
                "CREATE INDEX IF NOT EXISTS idx_model_configurations_room ON model_configurations (room, created_at DESC);",
                "CREATE INDEX IF NOT EXISTS idx_model_configurations_active ON model_configurations (is_active, created_at DESC) WHERE is_active = TRUE;",
                
                # room_occupancy indices
                "CREATE INDEX IF NOT EXISTS idx_room_occupancy_room ON room_occupancy (room, timestamp DESC);"
            ]
            
            for index_sql in indices:
                try:
                    await conn.execute(text(index_sql))
                    logger.info(f"Index created: {index_sql.split('IF NOT EXISTS ')[1].split()[0]}")
                except Exception as e:
                    logger.warning(f"Index creation failed (may already exist): {e}")
        
        print("  âœ“ Complete database schema created with all required tables and indices")
    
    async def _initialize_learning(self):
        """Initialize adaptive learning components"""
        
        # Initialize pattern discovery
        print("  - Setting up pattern discovery...")
        self.components['pattern_discovery'] = PatternDiscovery()
        
        # Initialize main prediction engine
        print("  - Setting up adaptive prediction engine...")
        predictor_config = self.config.config.copy()
        predictor_config['timeseries_db'] = self.components['timeseries_db']
        predictor_config['feature_store'] = self.components['feature_store']
        self.components['predictor'] = AdaptiveOccupancyPredictor(
            config=predictor_config
        )
        await self.components['predictor'].initialize()
        
        # Initialize cat detection
        print("  - Setting up adaptive cat detector...")
        self.components['cat_detector'] = AdaptiveCatDetector()
        
        print("  âœ“ Learning components initialized")
    
    async def _import_historical_data(self):
        """Import 180 days of historical data using existing historical_import.py"""
        
        print("  - Checking for existing historical data...")
        
        # Check if we already have sufficient historical data
        async with self.components['timeseries_db'].engine.begin() as conn:
            from sqlalchemy import text
            
            # Check total event count
            result = await conn.execute(text("SELECT COUNT(*) FROM sensor_events"))
            total_events = result.fetchone()[0]
            
            # Check date range
            result = await conn.execute(text("SELECT MIN(timestamp), MAX(timestamp) FROM sensor_events"))
            min_date, max_date = result.fetchone()
            
            if min_date and max_date:
                date_range_days = (max_date - min_date).days
                print(f"    - Found {total_events:,} existing events")
                print(f"    - Date range: {date_range_days} days ({min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')})")
                
                # If we have substantial historical data (>500k events, >150 days), skip import
                if total_events > 500000 and date_range_days > 150:
                    print("  âœ“ Sufficient historical data already exists - skipping import")
                    return
        
        print("  - Running historical data import (180 days)...")
        
        # Run the existing historical_import.py script
        script_path = Path(__file__).parent / "historical_import.py"
        config_path = Path(__file__).parent.parent / "config" / "system.yaml"
        
        try:
            # Execute the historical import script
            result = subprocess.run([
                sys.executable, str(script_path),
                "--days", "180",
                "--config", str(config_path)
            ], capture_output=True, text=True, cwd=Path(__file__).parent.parent)
            
            if result.returncode == 0:
                print("  âœ“ Historical data import completed successfully")
                if result.stdout:
                    # Show summary from import
                    lines = result.stdout.strip().split('\n')
                    summary_lines = [line for line in lines if 'Total' in line or 'imported' in line or 'âœ…' in line]
                    for line in summary_lines[-3:]:  # Show last 3 summary lines
                        print(f"    {line}")
            else:
                logger.error(f"Historical import failed: {result.stderr}")
                raise RuntimeError(f"Historical data import failed: {result.stderr}")
                
        except Exception as e:
            logger.error(f"Error running historical import: {e}")
            raise RuntimeError(f"Failed to import historical data: {e}")
    
    async def _configure_combined_spaces(self):
        """Configure unified living/kitchen space per CLAUDE.md"""
        
        print("  - Merging living room and kitchen into unified space...")
        
        # Update room mappings in database for unified space
        async with self.components['timeseries_db'].engine.begin() as conn:
            from sqlalchemy import text
            
            # Update sensor events (main historical data)
            result = await conn.execute(text("""
                UPDATE sensor_events 
                SET room = 'living_kitchen' 
                WHERE room IN ('livingroom', 'kitchen')
            """))
            updated_events = result.rowcount
            print(f"    - Updated {updated_events:,} sensor events")
            
            # Update occupancy data if table exists and has data
            try:
                result = await conn.execute(text("""
                    UPDATE room_occupancy 
                    SET room = 'living_kitchen' 
                    WHERE room IN ('livingroom', 'kitchen')
                """))
                updated_occupancy = result.rowcount
                print(f"    - Updated {updated_occupancy:,} occupancy records")
            except Exception as e:
                # Table might not exist or be empty during bootstrap
                print(f"    - No occupancy data to update (table may not exist yet)")
                logger.debug(f"Occupancy update skipped: {e}")
        
        print("  âœ“ Combined living/kitchen space configured")
    
    async def _discover_patterns(self):
        """
        Discover patterns for each room type following CLAUDE.md exactly
        """
        
        print("  - Discovering multi-zone patterns...")
        
        # Discover patterns for each room type as specified in CLAUDE.md
        room_types = {
            'living_kitchen': 'multizone',  # Combined space
            'bedroom': 'multizone',
            'office': 'multizone', 
            'bathroom': 'bathroom',         # Special bathroom logic
            'small_bathroom': 'bathroom',   # Special bathroom logic
            'hallways': 'transition'        # Transition patterns
        }
        
        for room, room_type in room_types.items():
            try:
                if room_type == 'multizone':
                    patterns = await self.components['pattern_discovery'].discover_multizone_patterns(room)
                elif room_type == 'bathroom':
                    patterns = await self.components['pattern_discovery'].discover_bathroom_patterns([room])
                elif room_type == 'transition':
                    patterns = await self.components['pattern_discovery'].discover_transition_patterns(room)
                
                print(f"    - {room}: {len(patterns.get('patterns', []))} patterns discovered")
                
            except Exception as e:
                logger.warning(f"Pattern discovery failed for {room}: {e}")
                print(f"    - {room}: pattern discovery failed ({e})")
        
        print("  âœ“ Pattern discovery completed")
    
    async def _initialize_person_learning(self):
        """Initialize person-specific learning for Anca and Vladimir"""
        
        print("  - Setting up person-specific learning...")
        
        # Initialize learning for both people as specified in CLAUDE.md
        persons = ['anca', 'vladimir']
        
        for person in persons:
            # Initialize person-specific patterns in cat detector
            if person not in self.components['cat_detector'].person_specific_patterns:
                self.components['cat_detector'].person_specific_patterns[person] = {
                    'common_zones': set(),
                    'movement_speed': self.components['cat_detector'].person_specific_patterns[person]['movement_speed'] if person in self.components['cat_detector'].person_specific_patterns else None
                }
            
            # Cache person learning initialization using correct Redis method
            person_data = {
                'initialized_at': datetime.now().isoformat(),
                'person': person,
                'learning_enabled': True,
                'patterns_discovered': 0
            }
            
            await self.components['feature_store'].store_model_state(
                f"person_learning_{person}_initialized",
                person_data
            )
            
            print(f"    - Initialized learning for {person}")
        
        print("  âœ“ Person-specific learning initialized")
    
    async def _start_continuous_learning(self):
        """Start the continuous learning system"""
        
        print("  - Starting adaptive learning system...")
        
        # The continuous learning is handled by the main system
        # This validates that the predictor is ready for continuous operation
        
        rooms_to_predict = [
            'living_kitchen',  # Combined space
            'bedroom',
            'office', 
            'bathroom',
            'small_bathroom',
            'guest_bedroom'
        ]
        
        # Validate predictor readiness for each room
        for room in rooms_to_predict:
            try:
                # Check if predictor can handle this room
                await self.components['predictor'].get_room_predictor(room)
                print(f"    - {room}: predictor ready")
            except Exception as e:
                logger.warning(f"Predictor not ready for {room}: {e}")
                print(f"    - {room}: predictor initialization pending")
        
        print("  âœ“ Continuous learning system ready")
    
    async def _setup_ha_integration(self):
        """Set up Home Assistant integration"""
        
        print("  - Creating Home Assistant integration...")
        
        # Initialize HA integration using existing DynamicHAIntegration
        ha_api = self.components.get('ha_api')  # Will be None initially, that's OK
        self.components['ha_integration'] = DynamicHAIntegration(
            ha_api=ha_api,
            timeseries_db=self.components['timeseries_db'],
            feature_store=self.components['feature_store']
        )
        await self.components['ha_integration'].initialize()
        
        # Create prediction entities for each room
        rooms_to_predict = [
            'living_kitchen',    # Combined space
            'bedroom',
            'office',
            'bathroom', 
            'small_bathroom',
            'guest_bedroom'
        ]
        
        for room in rooms_to_predict:
            # Default prediction horizons
            horizons = [15, 120]  # 15 min, 2 hours as per CLAUDE.md
            
            for horizon in horizons:
                entity_id = f"sensor.occupancy_{room}_{horizon}min"
                print(f"    - Configured predictor: {entity_id}")
            
            # Create trend and anomaly entities
            print(f"    - Configured trend sensor: sensor.occupancy_trend_{room}")
            print(f"    - Configured anomaly sensor: binary_sensor.occupancy_anomaly_{room}")
        
        print("  âœ“ Home Assistant integration configured")
    
    async def _validate_system(self):
        """Validate complete system readiness"""
        
        print("  - Validating database connectivity...")
        
        # Test database connection by getting stats
        stats = await self.components['timeseries_db'].get_database_stats()
        if not stats:
            raise RuntimeError("Database connection failed")
        
        print("  - Validating Redis connectivity...")
        
        # Test Redis connection using correct Redis methods
        test_data = {"status": "success", "timestamp": datetime.now().isoformat()}
        await self.components['feature_store'].store_model_state("bootstrap_test", test_data)
        
        # Verify the data was stored
        retrieved_data = await self.components['feature_store'].get_model_state("bootstrap_test")
        if not retrieved_data or retrieved_data.get("status") != "success":
            raise RuntimeError("Redis connection failed")
        
        print("  - Validating model storage...")
        
        # Test model storage
        storage_stats = await self.components['model_store'].get_storage_stats()
        if 'storage_path' not in storage_stats:
            raise RuntimeError("Model storage not accessible")
        
        print("  - Validating historical data...")
        
        # Check that historical data was imported using existing method
        try:
            recent_events = await self.components['timeseries_db'].get_recent_events(minutes=10080)  # 1 week
            count = len(recent_events) if recent_events else 0
            
            if count == 0:
                logger.warning("No recent historical data found - import may have failed or be in progress")
                print("    - No recent data found (import may be in progress)")
            else:
                print(f"    - Found {count:,} recent events")
        except Exception as e:
            logger.warning(f"Could not validate historical data: {e}")
            print("    - Historical data validation skipped")
        
        print("  - Validating sensor configuration...")
        
        # Count total configured sensors
        total_sensors = sum(len(sensors) for sensors in self.sensor_groups.values())
        print(f"    - {total_sensors} sensors configured across all types")
        
        if total_sensors < 90:  # Should have ~98 sensors
            logger.warning(f"Only {total_sensors} sensors configured, expected ~98")
        
        print("  âœ“ System validation passed")
    
    async def cleanup(self):
        """Clean up resources"""
        
        for component in self.components.values():
            if hasattr(component, 'shutdown'):
                await component.shutdown()
            elif hasattr(component, 'close'):
                await component.close()


async def main():
    """Main bootstrap entry point"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description='Complete HA Intent Predictor system bootstrap')
    parser.add_argument('--config', type=str, default='config/system.yaml', 
                       help='Configuration file path')
    parser.add_argument('--force', action='store_true', 
                       help='Force bootstrap even if system appears initialized')
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Create logs directory
    os.makedirs('logs', exist_ok=True)
    
    bootstrap = CompleteSystemBootstrap(args.config)
    
    try:
        await bootstrap.bootstrap_system()
        
        print("\nðŸŽ‰ Bootstrap completed successfully!")
        print("The system is now ready for continuous operation.")
        print("You can start the main system with: python main.py")
        
    except Exception as e:
        logger.error(f"Bootstrap failed: {e}")
        print(f"\nâŒ Bootstrap failed: {e}")
        sys.exit(1)
    
    finally:
        await bootstrap.cleanup()


if __name__ == "__main__":
    asyncio.run(main())